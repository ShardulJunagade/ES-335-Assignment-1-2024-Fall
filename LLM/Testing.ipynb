{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "from langchain_groq import ChatGroq  # Assuming you are using the langchain_groq package\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the model mapping\n",
    "groq_models = {\n",
    "    \"llama3-70b\": \"llama3-70b-8192\",\n",
    "    \"mixtral\": \"mixtral-8x7b-32768\",\n",
    "    \"gemma-7b\": \"gemma-7b-it\",\n",
    "    \"llama3.1-70b\": \"llama-3.1-70b-versatile\",\n",
    "    \"llama3-8b\": \"llama3-8b-8192\",\n",
    "    \"llama3.1-8b\": \"llama-3.1-8b-instant\",\n",
    "    \"gemma-9b\": \"gemma2-9b-it\"\n",
    "}\n",
    "\n",
    "activity_labels = {\n",
    "    1: \"WALKING\",\n",
    "    2: \"WALKING_UPSTAIRS\",\n",
    "    3: \"WALKING_DOWNSTAIRS\",\n",
    "    4: \"SITTING\",\n",
    "    5: \"STANDING\",\n",
    "    6: \"LAYING\"\n",
    "}\n",
    "reverse_activity_labels = {v: k for k, v in activity_labels.items()}\n",
    "\n",
    "print(\"Activity Labels Dictionary: \", reverse_activity_labels)\n",
    "\n",
    "# Load datasets\n",
    "X_train = np.load('../FinalDataset/X_train.npy')\n",
    "X_test = np.load('../FinalDataset/X_test.npy')\n",
    "y_train = np.load('../FinalDataset/y_train.npy')\n",
    "y_test = np.load('../FinalDataset/y_test.npy')\n",
    "\n",
    "#############################################################################################\n",
    "import tsfel\n",
    "\n",
    "# Extract features using TSFEL\n",
    "cfg = tsfel.get_features_by_domain()  # Get all features by default\n",
    "X_train_features = tsfel.time_series_features_extractor(cfg, X_train, verbose=1, fs=50)\n",
    "X_test_features = tsfel.time_series_features_extractor(cfg, X_test, verbose=1, fs=50)\n",
    "print(\"Shape of train data after feature extraction using TSFEL:\", X_train_features.shape)\n",
    "print(\"Shape of test data after feature extraction using TSFEL:\", X_test_features.shape)\n",
    "\n",
    "# Remove highly correlated features\n",
    "correlated_features = tsfel.correlated_features(X_train_features)\n",
    "print(\"Highly correlated features (sample):\", correlated_features[:5])\n",
    "X_train_filtered = X_train_features.drop(correlated_features, axis=1)\n",
    "X_test_filtered = X_test_features.drop(correlated_features, axis=1)\n",
    "print(\"Shape of data after removing correlated features:\", X_train_filtered.shape, X_test_filtered.shape)\n",
    "\n",
    "# Remove low variance features\n",
    "variance_selector = VarianceThreshold(threshold=0)  # Default is 0\n",
    "X_train_reduced = variance_selector.fit_transform(X_train_filtered)\n",
    "X_test_reduced = variance_selector.transform(X_test_filtered)\n",
    "print(\"Shape of data after variance thresholding:\", X_train_reduced.shape, X_test_reduced.shape)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train_reduced)\n",
    "X_test_normalized = scaler.transform(X_test_reduced)\n",
    "print(\"Shape of data after normalization:\", X_train_normalized.shape, X_test_normalized.shape)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=6)\n",
    "X_train_pca = pca.fit_transform(X_train_normalized)\n",
    "X_test_pca = pca.transform(X_test_normalized)\n",
    "\n",
    "print(\"Shape of data after PCA:\", X_train_pca.shape)\n",
    "print(\"Shape of test data after PCA:\", X_test_pca.shape)\n",
    "\n",
    "##################################################################################################\n",
    "\n",
    "# Constants\n",
    "NUM_EXAMPLES_PER_CLASS = 4\n",
    "NUM_SAMPLES = 20\n",
    "MAX_RETRIES = 3\n",
    "RETRY_DELAY = 5\n",
    "\n",
    "# API Keys\n",
    "API_KEYS = [\n",
    "    \"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\"\n",
    "]\n",
    "api_key_index = 0\n",
    "\n",
    "def get_next_api_key():\n",
    "    global api_key_index\n",
    "    key = API_KEYS[api_key_index]\n",
    "    api_key_index = (api_key_index + 1) % len(API_KEYS)\n",
    "    return key\n",
    "\n",
    "def format_data_as_string(data):\n",
    "    return str(data.tolist())\n",
    "\n",
    "def create_few_shot_examples(X, y, activity_labels, num_examples_per_class, num_samples_per_class):\n",
    "    examples = []\n",
    "    for activity, label in activity_labels.items():\n",
    "        indices = np.where(y == activity)[0]\n",
    "        selected_indices = np.random.choice(indices, num_examples_per_class + num_samples_per_class, replace=False)\n",
    "        for idx in selected_indices:\n",
    "            example_data = X[idx]\n",
    "            example_str = format_data_as_string(example_data)\n",
    "            examples.append((example_str, label))\n",
    "    return shuffle(examples)\n",
    "\n",
    "def create_prompt(examples, data_str):\n",
    "    example_strs = \"\\n\".join([f\"    - Example {i+1}: {ex[0]} -> {ex[1]}\" for i, ex in enumerate(examples)])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are a highly trained human activity classification model.\n",
    "\n",
    "    Your task is to analyze the given accelerometer data and classify the human activity into one of the following categories:\n",
    "    - WALKING\n",
    "    - WALKING_UPSTAIRS\n",
    "    - WALKING_DOWNSTAIRS\n",
    "    - SITTING\n",
    "    - STANDING\n",
    "    - LAYING\n",
    "\n",
    "    Here is the accelerometer data provided:\n",
    "    - You have 500 readings, each containing three accelerometer values: (acceleration_x, acceleration_y, acceleration_z).\n",
    "    - The data is collected over a 10-second period at a sampling rate of 50 Hz, which gives 500 readings.\n",
    "    - I have used the TSFEL library to reduce the dataset to 116 features.\n",
    "    Data Format:\n",
    "    - The data is provided as a nested list. Each inner list represents a single reading: (acceleration_x, acceleration_y, acceleration_z).\n",
    "    - The x component represents depth direction, measured in terms of g-force.\n",
    "    - The y component represents sideways direction, measured in terms of g-force.\n",
    "    - The z component represents forward direction, measured in terms of g-force.\n",
    "\n",
    "    Here are a few examples:{example_strs}\n",
    "\n",
    "    Please analyze the examples extensively and provide the most likely activity label for the below data from the list above.\n",
    "\n",
    "    Provide ONLY the classification label (from the given options above) as output.\n",
    "\n",
    "    Data: {data_str}\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "def predict_activity_labels(X_test, examples, activity_labels, reverse_activity_labels, model_name):\n",
    "    predictions = []\n",
    "    for i in range(NUM_SAMPLES):\n",
    "        test_example = X_test[i]\n",
    "        test_data_str = format_data_as_string(test_example)\n",
    "        prompt = create_prompt(examples, test_data_str)\n",
    "        \n",
    "        api_key = get_next_api_key()\n",
    "        llm = ChatGroq(model=groq_models[model_name], api_key=api_key, temperature=0)\n",
    "\n",
    "        for attempt in range(MAX_RETRIES):\n",
    "            try:\n",
    "                response = llm.invoke(prompt)\n",
    "                print(response.usage_metadata)\n",
    "                predicted_label = response.content.strip()\n",
    "                activity_number = reverse_activity_labels.get(predicted_label, -1)\n",
    "                actual_activity = activity_labels[y_test[i]]\n",
    "                print(f\"Predicted Activity: {predicted_label} | Actual Activity: {actual_activity}\")\n",
    "                predictions.append(activity_number)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}. Retrying in {RETRY_DELAY} seconds...\")\n",
    "                time.sleep(RETRY_DELAY)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Create balanced few-shot examples\n",
    "few_shot_examples = create_few_shot_examples(X_train_pca, y_train, activity_labels, NUM_EXAMPLES_PER_CLASS, NUM_SAMPLES)\n",
    "\n",
    "# Predict activities\n",
    "model_name = \"llama3.1-70b\"\n",
    "predicted_labels = predict_activity_labels(X_test_pca, few_shot_examples, activity_labels, reverse_activity_labels, model_name)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_predictions = sum(predicted_labels[i] == y_test[i] for i in range(NUM_SAMPLES))\n",
    "accuracy = correct_predictions / NUM_SAMPLES\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Accuracy Percentage: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "from langchain_groq import ChatGroq  # Assuming you are using the langchain_groq package\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Define the model mapping\n",
    "groq_models = {\n",
    "    \"llama3-70b\": \"llama3-70b-8192\",\n",
    "    \"mixtral\": \"mixtral-8x7b-32768\",\n",
    "    \"gemma-7b\": \"gemma-7b-it\",\n",
    "    \"llama3.1-70b\": \"llama-3.1-70b-versatile\",\n",
    "    \"llama3-8b\": \"llama3-8b-8192\",\n",
    "    \"llama3.1-8b\": \"llama-3.1-8b-instant\",\n",
    "    \"gemma-9b\": \"gemma2-9b-it\"\n",
    "}\n",
    "\n",
    "activity_labels = {\n",
    "    1: \"WALKING\",\n",
    "    2: \"WALKING_UPSTAIRS\",\n",
    "    3: \"WALKING_DOWNSTAIRS\",\n",
    "    4: \"SITTING\",\n",
    "    5: \"STANDING\",\n",
    "    6: \"LAYING\"\n",
    "}\n",
    "reverse_activity_labels = {v: k for k, v in activity_labels.items()}\n",
    "\n",
    "print(\"Activity Labels Dictionary: \", reverse_activity_labels)\n",
    "\n",
    "# Load datasets\n",
    "X_train = np.load('../FinalDataset/X_train.npy')\n",
    "X_test = np.load('../FinalDataset/X_test.npy')\n",
    "y_train = np.load('../FinalDataset/y_train.npy')\n",
    "y_test = np.load('../FinalDataset/y_test.npy')\n",
    "\n",
    "# Feature extraction and processing steps...\n",
    "\n",
    "import tsfel\n",
    "\n",
    "# Extract features using TSFEL\n",
    "cfg = tsfel.get_features_by_domain()  # Get all features by default\n",
    "X_train_features = tsfel.time_series_features_extractor(cfg, X_train, verbose=1, fs=50)\n",
    "X_test_features = tsfel.time_series_features_extractor(cfg, X_test, verbose=1, fs=50)\n",
    "\n",
    "# Remove highly correlated features\n",
    "correlated_features = tsfel.correlated_features(X_train_features)\n",
    "X_train_filtered = X_train_features.drop(correlated_features, axis=1)\n",
    "X_test_filtered = X_test_features.drop(correlated_features, axis=1)\n",
    "\n",
    "# Remove low variance features\n",
    "variance_selector = VarianceThreshold(threshold=0)\n",
    "X_train_reduced = variance_selector.fit_transform(X_train_filtered)\n",
    "X_test_reduced = variance_selector.transform(X_test_filtered)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train_reduced)\n",
    "X_test_normalized = scaler.transform(X_test_reduced)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train_normalized)\n",
    "X_test_pca = pca.transform(X_test_normalized)\n",
    "\n",
    "# Constants\n",
    "NUM_SAMPLES_PER_ACTIVITY = 3\n",
    "NUM_TRAINING_EXAMPLES = 18  # Total number of examples for few-shot learning\n",
    "MAX_RETRIES = 3\n",
    "RETRY_DELAY = 5\n",
    "\n",
    "# API Keys\n",
    "API_KEYS = [\n",
    "    \"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\"\n",
    "]\n",
    "api_key_index = 0\n",
    "\n",
    "def get_next_api_key():\n",
    "    global api_key_index\n",
    "    key = API_KEYS[api_key_index]\n",
    "    api_key_index = (api_key_index + 1) % len(API_KEYS)\n",
    "    return key\n",
    "\n",
    "def format_data_as_string(data):\n",
    "    return str(data.tolist())\n",
    "\n",
    "def create_few_shot_examples(X_train_pca, y_train, activity_labels, num_samples_per_activity):\n",
    "    few_shot_examples = []\n",
    "    for activity, label in activity_labels.items():\n",
    "        indices = np.where(y_train == activity)[0]\n",
    "        selected_indices = np.random.choice(indices, num_samples_per_activity, replace=False)\n",
    "        for idx in selected_indices:\n",
    "            example_data = X_train_pca[idx]\n",
    "            example_str = format_data_as_string(example_data)\n",
    "            few_shot_examples.append((example_str, label))\n",
    "    return shuffle(few_shot_examples)\n",
    "\n",
    "def create_prompt(few_shot_examples, data_str):\n",
    "    example_strs = \"\\n\".join([f\"    - Example {i+1}: {ex[0]} -> {ex[1]}\" for i, ex in enumerate(few_shot_examples)])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are a highly trained human activity classification model.\n",
    "\n",
    "    Your task is to analyze the given accelerometer data and classify the human activity into one of the following categories:\n",
    "    - WALKING\n",
    "    - WALKING_UPSTAIRS\n",
    "    - WALKING_DOWNSTAIRS\n",
    "    - SITTING\n",
    "    - STANDING\n",
    "    - LAYING\n",
    "\n",
    "    Here is the accelerometer data provided:\n",
    "    - You have 500 readings, each containing three accelerometer values: (acceleration_x, acceleration_y, acceleration_z).\n",
    "    - The data is collected over a 10-second period at a sampling rate of 50 Hz, which gives 500 readings.\n",
    "    - I have used the TSFEL library to reduce the dataset to 116 features.\n",
    "    Data Format:\n",
    "    - The data is provided as a nested list. Each inner list represents a single reading: (acceleration_x, acceleration_y, acceleration_z).\n",
    "    - The x component represents depth direction, measured in terms of g-force.\n",
    "    - The y component represents sideways direction, measured in terms of g-force.\n",
    "    - The z component represents forward direction, measured in terms of g-force.\n",
    "\n",
    "    Here are a few examples:{example_strs}\n",
    "\n",
    "    Please analyze the examples extensively and provide the most likely activity label for the below data from the list above.\n",
    "\n",
    "    Provide ONLY the classification label (from the given options above) as output.\n",
    "\n",
    "    Data: {data_str}\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "def create_balanced_subset(X_test_pca, y_test, activity_labels):\n",
    "    min_samples_per_class = min(np.sum(y_test == activity) for activity in activity_labels.keys())\n",
    "    balanced_X_test = []\n",
    "    balanced_y_test = []\n",
    "    \n",
    "    for activity in activity_labels.keys():\n",
    "        indices = np.where(y_test == activity)[0]\n",
    "        selected_indices = np.random.choice(indices, min_samples_per_class, replace=False)\n",
    "        balanced_X_test.extend(X_test_pca[selected_indices])\n",
    "        balanced_y_test.extend(y_test[selected_indices])\n",
    "    \n",
    "    return np.array(balanced_X_test), np.array(balanced_y_test)\n",
    "\n",
    "def predict_activity_labels(X_test_pca, y_test, few_shot_examples, activity_labels, reverse_activity_labels, model_name):\n",
    "    predictions = []\n",
    "    balanced_X_test, balanced_y_test = create_balanced_subset(X_test_pca, y_test, activity_labels)\n",
    "    \n",
    "    # Use exactly 3 samples from each activity for prediction\n",
    "    test_subset_indices = []\n",
    "    for activity in activity_labels.keys():\n",
    "        indices = np.where(balanced_y_test == activity)[0]\n",
    "        selected_indices = np.random.choice(indices, NUM_SAMPLES_PER_ACTIVITY, replace=False)\n",
    "        test_subset_indices.extend(selected_indices)\n",
    "    \n",
    "    test_subset = balanced_X_test[test_subset_indices]\n",
    "    test_subset_labels = balanced_y_test[test_subset_indices]\n",
    "    \n",
    "    for i in range(len(test_subset)):\n",
    "        test_example = test_subset[i]\n",
    "        test_data_str = format_data_as_string(test_example)\n",
    "        prompt = create_prompt(few_shot_examples, test_data_str)\n",
    "        \n",
    "        api_key = get_next_api_key()\n",
    "        llm = ChatGroq(model=groq_models[model_name], api_key=api_key, temperature=0)\n",
    "\n",
    "        for attempt in range(MAX_RETRIES):\n",
    "            try:\n",
    "                response = llm.invoke(prompt)\n",
    "                print(response.usage_metadata)\n",
    "                predicted_label = response.content.strip()\n",
    "                activity_number = reverse_activity_labels.get(predicted_label, -1)\n",
    "                actual_activity = activity_labels[test_subset_labels[i]]\n",
    "                print(f\"Predicted Activity: {predicted_label} | Actual Activity: {actual_activity}\")\n",
    "                predictions.append(activity_number)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}. Retrying in {RETRY_DELAY} seconds...\")\n",
    "                time.sleep(RETRY_DELAY)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Create balanced few-shot examples\n",
    "few_shot_examples = create_few_shot_examples(X_train_pca, y_train, activity_labels, NUM_SAMPLES_PER_ACTIVITY)\n",
    "\n",
    "# Predict activities\n",
    "model_name = \"llama3.1-70b\"\n",
    "predicted_labels = predict_activity_labels(X_test_pca, y_test, few_shot_examples, activity_labels, reverse_activity_labels, model_name)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_predictions = sum(predicted_labels[i] == y_test[i] for i in range(len(predicted_labels)))\n",
    "accuracy = correct_predictions / len(predicted_labels)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Accuracy Percentage: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from langchain_groq import ChatGroq  # Assuming you are using the langchain_groq package\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Define the model mapping\n",
    "groq_models = {\n",
    "    \"llama3-70b\": \"llama3-70b-8192\",\n",
    "    \"mixtral\": \"mixtral-8x7b-32768\",\n",
    "    \"gemma-7b\": \"gemma-7b-it\",\n",
    "    \"llama3.1-70b\": \"llama-3.1-70b-versatile\",\n",
    "    \"llama3-8b\": \"llama3-8b-8192\",\n",
    "    \"llama3.1-8b\": \"llama-3.1-8b-instant\",\n",
    "    \"gemma-9b\": \"gemma2-9b-it\"\n",
    "}\n",
    "\n",
    "activity_labels = {\n",
    "    1: \"WALKING\",\n",
    "    2: \"WALKING_UPSTAIRS\",\n",
    "    3: \"WALKING_DOWNSTAIRS\",\n",
    "    4: \"SITTING\",\n",
    "    5: \"STANDING\",\n",
    "    6: \"LAYING\"\n",
    "}\n",
    "reverse_activity_labels = {v: k for k, v in activity_labels.items()}\n",
    "\n",
    "print(\"Activity Labels Dictionary: \", reverse_activity_labels)\n",
    "\n",
    "# Load datasets\n",
    "X_train = np.load('../FinalDataset/X_train.npy')\n",
    "X_test = np.load('../FinalDataset/X_test.npy')\n",
    "y_train = np.load('../FinalDataset/y_train.npy')\n",
    "y_test = np.load('../FinalDataset/y_test.npy')\n",
    "\n",
    "# Feature extraction and processing steps...\n",
    "\n",
    "import tsfel\n",
    "\n",
    "# Extract features using TSFEL\n",
    "cfg = tsfel.get_features_by_domain()  # Get all features by default\n",
    "X_train_features = tsfel.time_series_features_extractor(cfg, X_train, verbose=1, fs=50)\n",
    "X_test_features = tsfel.time_series_features_extractor(cfg, X_test, verbose=1, fs=50)\n",
    "\n",
    "# Remove highly correlated features\n",
    "correlated_features = tsfel.correlated_features(X_train_features)\n",
    "X_train_filtered = X_train_features.drop(correlated_features, axis=1)\n",
    "X_test_filtered = X_test_features.drop(correlated_features, axis=1)\n",
    "\n",
    "# Remove low variance features\n",
    "variance_selector = VarianceThreshold(threshold=0)\n",
    "X_train_reduced = variance_selector.fit_transform(X_train_filtered)\n",
    "X_test_reduced = variance_selector.transform(X_test_filtered)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train_reduced)\n",
    "X_test_normalized = scaler.transform(X_test_reduced)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train_normalized)\n",
    "X_test_pca = pca.transform(X_test_normalized)\n",
    "\n",
    "# Constants\n",
    "NUM_SAMPLES_PER_ACTIVITY = 3\n",
    "NUM_TRAINING_EXAMPLES = 18 # Total number of examples for few-shot learning\n",
    "MAX_RETRIES = 3\n",
    "RETRY_DELAY = 5\n",
    "\n",
    "# # API Keys\n",
    "# API_KEYS = [\n",
    "#     \"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\"\n",
    "# ]\n",
    "\n",
    "API_KEYS = [\"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\",\n",
    "               \"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\",\n",
    "               \"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\",\n",
    "                \"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\"\n",
    "]\n",
    "api_key_index = 0\n",
    "\n",
    "def get_next_api_key():\n",
    "    global api_key_index\n",
    "    key = API_KEYS[api_key_index]\n",
    "    api_key_index = (api_key_index + 1) % len(API_KEYS)\n",
    "    return key\n",
    "\n",
    "def format_data_as_string(data):\n",
    "    return str(data.tolist())\n",
    "\n",
    "def create_few_shot_examples(X_train_pca, y_train, activity_labels, num_samples_per_activity):\n",
    "    few_shot_examples = []\n",
    "    for activity, label in activity_labels.items():\n",
    "        indices = np.where(y_train == activity)[0]\n",
    "        selected_indices = np.random.choice(indices, num_samples_per_activity, replace=False)\n",
    "        for idx in selected_indices:\n",
    "            example_data = X_train_pca[idx]\n",
    "            example_str = format_data_as_string(example_data)\n",
    "            few_shot_examples.append((example_str, label))\n",
    "    return shuffle(few_shot_examples)\n",
    "\n",
    "def create_prompt(few_shot_examples, data_str):\n",
    "    example_strs = \"\\n\".join([f\"    - Example {i+1}: {ex[0]} -> {ex[1]}\" for i, ex in enumerate(few_shot_examples)])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are a highly trained human activity classification model.\n",
    "\n",
    "    Your task is to analyze the given accelerometer data and classify the human activity into one of the following categories:\n",
    "    - WALKING\n",
    "    - WALKING_UPSTAIRS\n",
    "    - WALKING_DOWNSTAIRS\n",
    "    - SITTING\n",
    "    - STANDING\n",
    "    - LAYING\n",
    "\n",
    "    Here is the accelerometer data provided:\n",
    "    - You have 500 readings, each containing three accelerometer values: (acceleration_x, acceleration_y, acceleration_z).\n",
    "    - The data is collected over a 10-second period at a sampling rate of 50 Hz, which gives 500 readings.\n",
    "    - I have used the TSFEL library to reduce the dataset to 116 features.\n",
    "    Data Format:\n",
    "    - The data is provided as a nested list. Each inner list represents a single reading: (acceleration_x, acceleration_y, acceleration_z).\n",
    "    - The x component represents depth direction, measured in terms of g-force.\n",
    "    - The y component represents sideways direction, measured in terms of g-force.\n",
    "    - The z component represents forward direction, measured in terms of g-force.\n",
    "\n",
    "    Here are a few examples:{example_strs}\n",
    "\n",
    "    Please analyze the examples extensively and provide the most likely activity label for the below data from the list above.\n",
    "\n",
    "    Provide ONLY the classification label (from the given options above) as output.\n",
    "\n",
    "    Data: {data_str}\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "def create_balanced_subset(X_test_pca, y_test, activity_labels):\n",
    "    min_samples_per_class = min(np.sum(y_test == activity) for activity in activity_labels.keys())\n",
    "    balanced_X_test = []\n",
    "    balanced_y_test = []\n",
    "    \n",
    "    for activity in activity_labels.keys():\n",
    "        indices = np.where(y_test == activity)[0]\n",
    "        selected_indices = np.random.choice(indices, min_samples_per_class, replace=False)\n",
    "        balanced_X_test.extend(X_test_pca[selected_indices])\n",
    "        balanced_y_test.extend(y_test[selected_indices])\n",
    "    \n",
    "    return np.array(balanced_X_test), np.array(balanced_y_test)\n",
    "\n",
    "def predict_activity_labels(X_test_pca, y_test, few_shot_examples, activity_labels, reverse_activity_labels, model_name):\n",
    "    predictions = []\n",
    "    balanced_X_test, balanced_y_test = create_balanced_subset(X_test_pca, y_test, activity_labels)\n",
    "    \n",
    "    # Use exactly 3 samples from each activity for prediction\n",
    "    test_subset_indices = []\n",
    "    for activity in activity_labels.keys():\n",
    "        indices = np.where(balanced_y_test == activity)[0]\n",
    "        selected_indices = np.random.choice(indices, NUM_SAMPLES_PER_ACTIVITY, replace=False)\n",
    "        test_subset_indices.extend(selected_indices)\n",
    "    \n",
    "    test_subset = balanced_X_test[test_subset_indices]\n",
    "    test_subset_labels = balanced_y_test[test_subset_indices]\n",
    "    \n",
    "    for i in range(len(test_subset)):\n",
    "        test_example = test_subset[i]\n",
    "        test_data_str = format_data_as_string(test_example)\n",
    "        prompt = create_prompt(few_shot_examples, test_data_str)\n",
    "        \n",
    "        api_key = get_next_api_key()\n",
    "        llm = ChatGroq(model=groq_models[model_name], api_key=api_key, temperature=0)\n",
    "\n",
    "        for attempt in range(MAX_RETRIES):\n",
    "            try:\n",
    "                response = llm.invoke(prompt)\n",
    "                print(response.usage_metadata)\n",
    "                predicted_label = response.content.strip()\n",
    "                activity_number = reverse_activity_labels.get(predicted_label, -1)\n",
    "                actual_activity = activity_labels[test_subset_labels[i]]\n",
    "                print(f\"Predicted Activity: {predicted_label} | Actual Activity: {actual_activity}\")\n",
    "                predictions.append(activity_number)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}. Retrying in {RETRY_DELAY} seconds...\")\n",
    "                time.sleep(RETRY_DELAY)\n",
    "    \n",
    "    return np.array(predictions), balanced_y_test[test_subset_indices]\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, activity_labels):\n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(activity_labels.keys()))\n",
    "    \n",
    "    # Create a heatmap for the confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(activity_labels.values()))\n",
    "    disp.plot(cmap=plt.cm.Blues, values_format='d')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "# Create balanced few-shot examples\n",
    "few_shot_examples = create_few_shot_examples(X_train_pca, y_train, activity_labels, NUM_SAMPLES_PER_ACTIVITY)\n",
    "\n",
    "# Predict activities\n",
    "model_name = \"llama3.1-70b\"\n",
    "predicted_labels, true_labels = predict_activity_labels(X_test_pca, y_test, few_shot_examples, activity_labels, reverse_activity_labels, model_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "correct_predictions = sum(predicted_labels[i] == true_labels[i] for i in range(len(predicted_labels)))\n",
    "accuracy = correct_predictions / len(predicted_labels)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Accuracy Percentage: {accuracy * 100:.2f}%\")\n",
    "# plot_confusion_matrix(true_labels, predicted_labels, activity_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "from langchain_groq import ChatGroq  # Assuming you are using the langchain_groq package\n",
    "\n",
    "\n",
    "# Define the model mapping\n",
    "groq_models = {\n",
    "    \"llama3-70b\": \"llama3-70b-8192\",\n",
    "    \"mixtral\": \"mixtral-8x7b-32768\",\n",
    "    \"gemma-7b\": \"gemma-7b-it\",\n",
    "    \"llama3.1-70b\": \"llama-3.1-70b-versatile\",\n",
    "    \"llama3-8b\": \"llama3-8b-8192\",\n",
    "    \"llama3.1-8b\": \"llama-3.1-8b-instant\",\n",
    "    \"gemma-9b\": \"gemma2-9b-it\"\n",
    "}\n",
    "\n",
    "activity_labels = {\n",
    "    1: \"WALKING\",\n",
    "    2: \"WALKING_UPSTAIRS\",\n",
    "    3: \"WALKING_DOWNSTAIRS\",\n",
    "    4: \"SITTING\",\n",
    "    5: \"STANDING\",\n",
    "    6: \"LAYING\"\n",
    "}\n",
    "reverse_activity_labels = {v: k for k, v in activity_labels.items()}\n",
    "\n",
    "print(\"Activity Labels Dictionary: \", reverse_activity_labels)\n",
    "\n",
    "# Load datasets\n",
    "X_train = np.load('../FinalDataset/X_train.npy')\n",
    "X_test = np.load('../FinalDataset/X_test.npy')\n",
    "y_train = np.load('../FinalDataset/y_train.npy')\n",
    "y_test = np.load('../FinalDataset/y_test.npy')\n",
    "# Load data\n",
    "# X_train_tsfel_reduced = np.load('../FinalDataset/X_train_tsfel_reduced.npy')\n",
    "# X_test_tsfel_reduced = np.load('../FinalDataset/X_test_tsfel_reduced.npy')\n",
    "\n",
    "print(\"Training data shape: \", X_train_tsfel_pca.shape)\n",
    "print(\"Testing data shape: \", X_test_tsfel_pca.shape)\n",
    "\n",
    "# Constants\n",
    "num_examples_per_class = 5\n",
    "num_samples = 20\n",
    "max_retries = 3\n",
    "retry_delay = 5\n",
    "\n",
    "def shuffle_data(X_test, y_test):\n",
    "    # Generate a random permutation of the indices\n",
    "    permutation = np.random.permutation(len(X_test))\n",
    "    \n",
    "    # Apply the permutation to shuffle both X_test and y_test\n",
    "    X_test_shuffled = X_test[permutation]\n",
    "    y_test_shuffled = y_test[permutation]\n",
    "    \n",
    "    return X_test_shuffled, y_test_shuffled\n",
    "\n",
    "# Example usage:\n",
    "X_test_tsfel_pca_shuffled, y_test_shuffled = shuffle_data(X_test_tsfel_pca, y_test)\n",
    "\n",
    "# API Keys and Index\n",
    "Groq_Tokens = [\"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\"]\n",
    "current_key_index = 0\n",
    "\n",
    "def get_next_api_key():\n",
    "    global current_key_index\n",
    "    api_key = Groq_Tokens[current_key_index]\n",
    "    current_key_index = (current_key_index + 1) % len(Groq_Tokens)\n",
    "    return api_key\n",
    "\n",
    "def format_data_for_prompt(data):\n",
    "    # Example formatting function; adjust as needed\n",
    "    return str(data.tolist())\n",
    "\n",
    "# def create_few_shot_examples(X_train, y_train, activity_dict, num_examples_per_class):\n",
    "#     examples = []\n",
    "#     for activity, label in activity_dict.items():\n",
    "#         class_indices = np.where(y_train == activity)[0]\n",
    "#         selected_indices = np.random.choice(class_indices, num_examples_per_class, replace=False)\n",
    "#         for idx in selected_indices:\n",
    "#             data_example = X_train[idx]\n",
    "#             data_str = format_data_for_prompt(data_example)\n",
    "#             examples.append((data_str, label))\n",
    "#     return shuffle(examples)\n",
    "\n",
    "def add_class_examples(X_train, y_train, activity_dict, num_samples_per_class=4):\n",
    "    examples = []\n",
    "    for activity, label in activity_dict.items():\n",
    "        class_indices = np.where(y_train == activity)[0]\n",
    "        class_samples = np.random.choice(class_indices, num_samples_per_class, replace=False)\n",
    "        for idx in class_samples:\n",
    "            data_example = X_train[idx]\n",
    "            data_str = format_data_for_prompt(data_example)\n",
    "            examples.append((data_str, label))\n",
    "    return shuffle(examples)\n",
    "\n",
    "def generate_prompt(examples, data_str):\n",
    "    output = \"\\n\"\n",
    "    for j, (example_input, example_output) in enumerate(examples):\n",
    "        output += f\"    - Example {j+1}: {example_input} -> {example_output}\\n\"\n",
    "    # print(output)\n",
    "    prompt = f\"\"\"\n",
    "    You are a highly trained human activity classification model.\n",
    "\n",
    "    Your task is to analyze the given accelerometer data and classify the human activity into one of the following categories:\n",
    "    - WALKING\n",
    "    - WALKING_UPSTAIRS\n",
    "    - WALKING_DOWNSTAIRS\n",
    "    - SITTING\n",
    "    - STANDING\n",
    "    - LAYING\n",
    "\n",
    "    Here is the processed data provided:\n",
    "    - You have two principal components: (principal_component_1, principal_component_2).\n",
    "    - The data was originally derived from accelerometer readings collected over a 10-second period at a sampling rate of 50 Hz.\n",
    "    - The principal components capture the most significant variance in the accelerometer data after dimensionality reduction using PCA.\n",
    "\n",
    "    Here are a few examples:{output}\n",
    "    \n",
    "    Please analyze the examples extensively and provide the most likely activity label for the below data from the list above.\n",
    "    \n",
    "    Provide ONLY the classification label (from the given options above) as output.\n",
    "\n",
    "    Data: {data_str}\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "def predict_activity(X_test, examples, activity_dict, activity_reverse_dict, model_name):\n",
    "    predictions = []\n",
    "    count = 0\n",
    "    for i in range(num_samples):\n",
    "        X_i = X_test[i]\n",
    "        data_str = format_data_for_prompt(X_i)\n",
    "        prompt = generate_prompt(examples, data_str)\n",
    "        \n",
    "        api_key = get_next_api_key()\n",
    "        llm = ChatGroq(model=groq_models[model_name], api_key=api_key, temperature=0)\n",
    "\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                answer = llm.invoke(prompt)\n",
    "                count+=answer.usage_metadata['total_tokens']\n",
    "                predicted_activity = answer.content.strip()\n",
    "                activity_number = activity_reverse_dict.get(predicted_activity, -1)\n",
    "                # Get actual label\n",
    "                actual_activity = activity_dict[y_test_shuffled[i]]\n",
    "                # Print predicted and actual activity\n",
    "                print(f\"Predicted Activity: {predicted_activity} | Actual Activity: {actual_activity}\")\n",
    "                predictions.append(activity_number)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}. Retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "    print(\"Total Tokens Used: \",count)\n",
    "    return predictions\n",
    "\n",
    "# Create balanced few-shot examples\n",
    "# examples = create_few_shot_examples(X_train_tsfel_pca, y_train, activity_dict, num_examples_per_class)\n",
    "examples = add_class_examples(X_train_tsfel_pca, y_train, activity_dict, num_samples_per_class=10)\n",
    "\n",
    "model_name = \"llama3.1-70b\"\n",
    "# Predict activities\n",
    "predictions = predict_activity(X_test_tsfel_pca_shuffled, examples, activity_dict, activity_reverse_dict, model_name)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct = sum(pred == y_test[i] for i, pred in enumerate(predictions))\n",
    "accuracy = correct / len(predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Accuracy Percentage: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from langchain_groq import ChatGroq\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "groq_models = {\n",
    "    \"llama3-70b\": \"llama3-70b-8192\",\n",
    "    \"mixtral\": \"mixtral-8x7b-32768\",\n",
    "    \"gemma-7b\": \"gemma-7b-it\",\n",
    "    \"llama3.1-70b\": \"llama-3.1-70b-versatile\",\n",
    "    \"llama3-8b\": \"llama3-8b-8192\",\n",
    "    \"llama3.1-8b\": \"llama-3.1-8b-instant\",\"\"\n",
    "    \"gemma-9b\": \"gemma2-9b-it\"\n",
    "}\n",
    "\n",
    "activity_labels = {\n",
    "    1: \"WALKING\",\n",
    "    2: \"WALKING_UPSTAIRS\",\n",
    "    3: \"WALKING_DOWNSTAIRS\",\n",
    "    4: \"SITTING\",\n",
    "    5: \"STANDING\",\n",
    "    6: \"LAYING\"\n",
    "}\n",
    "reverse_activity_labels = {v: k for k, v in activity_labels.items()}\n",
    "\n",
    "print(\"Activity Labels Dictionary: \", reverse_activity_labels)\n",
    "\n",
    "X_train = np.load('../FinalDataset/X_train.npy')\n",
    "X_test = np.load('../FinalDataset/X_test.npy')\n",
    "y_train = np.load('../FinalDataset/y_train.npy')\n",
    "y_test = np.load('../FinalDataset/y_test.npy')\n",
    "\n",
    "X_train = np.hstack(X_train)\n",
    "X_test = np.hstack(X_test)\n",
    "\n",
    "\n",
    "# Function to make API calls with retry logic for rate limit errors\n",
    "def make_api_call(prompt):\n",
    "    while True:\n",
    "        try:\n",
    "            response = llm.invoke(prompt)\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"Rate limit error: {e}. Retrying in 10 seconds...\")\n",
    "            time.sleep(10)\n",
    "\n",
    "# Set up Groq API credentials and model\n",
    "Groq_Token = \"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\"\n",
    "model_name = \"llama3.1-70b\"\n",
    "llm = ChatGroq(model=groq_models[model_name], api_key=Groq_Token, temperature=0)\n",
    "\n",
    "# Select even samples for few-shot learning examples\n",
    "samples_per_activity = 2\n",
    "few_shot_indices = []\n",
    "activity_count = {i: 0 for i in range(1, 7)}\n",
    "\n",
    "for i, label in enumerate(y_train):\n",
    "    if activity_count[label] < samples_per_activity:\n",
    "        few_shot_indices.append(i)\n",
    "        activity_count[label] += 1\n",
    "    if all(count == samples_per_activity for count in activity_count.values()):\n",
    "        break\n",
    "\n",
    "# Create few-shot examples\n",
    "few_shot_examples = [\n",
    "    {\"input\": X_train[i].tolist(), \"label\": y_train[i]} for i in few_shot_indices\n",
    "]\n",
    "\n",
    "# Function to create a few-shot learning prompt\n",
    "def create_few_shot_prompt(examples, query_input):\n",
    "    description = '''\n",
    "        You are a highly trained human activity classification model.\n",
    "        Each input is a vector containing numerical values that represent transformed features.\n",
    "        The data consists of 20 principal components extracted from the Human Activity Recognition dataset.\n",
    "        Your task is to classify the input vector into one of the following categories:\n",
    "        - 1: WALKING\n",
    "        - 2: WALKING_UPSTAIRS\n",
    "        - 3: WALKING_DOWNSTAIRS\n",
    "        - 4: SITTING \n",
    "        - 5: STANDING\n",
    "        - 6: LAYING\n",
    "\n",
    "        Here are a few examples:\\n\n",
    "    '''\n",
    "    prompt = description\n",
    "    for ex in examples:\n",
    "        example_input = \",\".join(map(str, ex['input']))\n",
    "        prompt += f\"Input: [{example_input}]\\nLabel: {ex['label']}\\n\\n\" \n",
    "        \n",
    "    query_input_str = \",\".join(map(str, query_input))\n",
    "    prompt += f\"Now, classify the following input vector and return ONLY the number.\\nInput: [{query_input_str}]\\nLabel: \"\n",
    "    return prompt\n",
    "\n",
    "# Select 3 samples from each activity for testing\n",
    "samples_per_activity = 3\n",
    "selected_indices = []\n",
    "activity_count = {i: 0 for i in range(1, 7)}\n",
    "\n",
    "for i, label in enumerate(y_test):\n",
    "    if activity_count[label] < samples_per_activity:\n",
    "        selected_indices.append(i)\n",
    "        activity_count[label] += 1\n",
    "    if all(count == samples_per_activity for count in activity_count.values()):\n",
    "        break\n",
    "\n",
    "# Store predictions and true labels\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# Test the selected cases\n",
    "for idx in selected_indices:\n",
    "    query_input = X_test[idx].tolist()\n",
    "    true_label = y_test[idx]\n",
    "    prompt = create_few_shot_prompt(few_shot_examples, query_input)\n",
    "    \n",
    "    # Make the API call and store the prediction\n",
    "    answer = make_api_call(prompt)\n",
    "    predicted_label = int(answer.content.strip())\n",
    "    predictions.append(predicted_label)\n",
    "    true_labels.append(true_label)\n",
    "    \n",
    "    # Print the predicted and true labels\n",
    "    print(f\"Test Case {idx + 1} ({activity_labels[true_label]}):\")\n",
    "    print(f\"Predicted label: {predicted_label}\")\n",
    "    print(f\"True label: {true_label}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Calculate and print accuracy\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Calculate and plot the confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predictions, labels=[1, 2, 3, 4, 5, 6])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['WALKING', 'WALKING_UPSTAIRS', 'WALKING_DOWNSTAIRS', 'SITTING', 'STANDING', 'LAYING'], \n",
    "            yticklabels=['WALKING', 'WALKING_UPSTAIRS', 'WALKING_DOWNSTAIRS', 'SITTING', 'STANDING', 'LAYING'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activity Labels Dictionary:  {'WALKING': 1, 'WALKING_UPSTAIRS': 2, 'WALKING_DOWNSTAIRS': 3, 'SITTING': 4, 'STANDING': 5, 'LAYING': 6}\n",
      "*** Feature extraction started ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "              <p>\n",
       "                  Progress: 0% Complete\n",
       "              <p/>\n",
       "              <progress\n",
       "                  value='0'\n",
       "                  max='126',\n",
       "                  style='width: 25%',\n",
       "              >\n",
       "                  0\n",
       "              </progress>\n",
       "\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Feature extraction finished ***\n",
      "*** Feature extraction started ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "              <p>\n",
       "                  Progress: 100% Complete\n",
       "              <p/>\n",
       "              <progress\n",
       "                  value='54'\n",
       "                  max='54',\n",
       "                  style='width: 25%',\n",
       "              >\n",
       "                  54\n",
       "              </progress>\n",
       "\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Feature extraction finished ***\n",
      "(126, 1152)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 47\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Remove highly correlated features\u001b[39;00m\n\u001b[0;32m     46\u001b[0m correlated_features \u001b[38;5;241m=\u001b[39m tsfel\u001b[38;5;241m.\u001b[39mcorrelated_features(X_train_tsfel)\n\u001b[1;32m---> 47\u001b[0m X_train_filtered \u001b[38;5;241m=\u001b[39m X_train_features\u001b[38;5;241m.\u001b[39mdrop(correlated_features, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     48\u001b[0m X_test_filtered \u001b[38;5;241m=\u001b[39m X_test_features\u001b[38;5;241m.\u001b[39mdrop(correlated_features, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Remove low variance features\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_features' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from langchain_groq import ChatGroq\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import tsfel\n",
    "\n",
    "groq_models = {\n",
    "    \"llama3-70b\": \"llama3-70b-8192\",\n",
    "    \"mixtral\": \"mixtral-8x7b-32768\",\n",
    "    \"gemma-7b\": \"gemma-7b-it\",\n",
    "    \"llama3.1-70b\": \"llama-3.1-70b-versatile\",\n",
    "    \"llama3-8b\": \"llama3-8b-8192\",\n",
    "    \"llama3.1-8b\": \"llama-3.1-8b-instant\",\"\"\n",
    "    \"gemma-9b\": \"gemma2-9b-it\"\n",
    "}\n",
    "\n",
    "activity_labels = {\n",
    "    1: \"WALKING\",\n",
    "    2: \"WALKING_UPSTAIRS\",\n",
    "    3: \"WALKING_DOWNSTAIRS\",\n",
    "    4: \"SITTING\",\n",
    "    5: \"STANDING\",\n",
    "    6: \"LAYING\"\n",
    "}\n",
    "reverse_activity_labels = {v: k for k, v in activity_labels.items()}\n",
    "\n",
    "print(\"Activity Labels Dictionary: \", reverse_activity_labels)\n",
    "\n",
    "X_train = np.load('../FinalDataset/X_train.npy')\n",
    "X_test = np.load('../FinalDataset/X_test.npy')\n",
    "y_train = np.load('../FinalDataset/y_train.npy')\n",
    "y_test = np.load('../FinalDataset/y_test.npy')\n",
    "\n",
    "# X_train = np.hstack(X_train)\n",
    "# X_test = np.hstack(X_test)\n",
    "\n",
    "cfg = tsfel.get_features_by_domain()  # Get all features by default\n",
    "X_train_tsfel = tsfel.time_series_features_extractor(cfg, X_train, verbose=1, fs=50)\n",
    "X_test_tsfel = tsfel.time_series_features_extractor(cfg, X_test, verbose=1, fs=50)\n",
    "print(X_train_tsfel.shape)\n",
    "\n",
    "# Remove highly correlated features\n",
    "correlated_features = tsfel.correlated_features(X_train_tsfel)\n",
    "X_train_filtered = X_train_features.drop(correlated_features, axis=1)\n",
    "X_test_filtered = X_test_features.drop(correlated_features, axis=1)\n",
    "\n",
    "# Remove low variance features\n",
    "variance_selector = VarianceThreshold(threshold=0)\n",
    "X_train_reduced = variance_selector.fit_transform(X_train_filtered)\n",
    "X_test_reduced = variance_selector.transform(X_test_filtered)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train_reduced)\n",
    "X_test_normalized = scaler.transform(X_test_reduced)\n",
    "\n",
    "# Apply PCA\n",
    "\n",
    "pca = PCA(n_components=4)\n",
    "X_train_pca_20 = pca.fit_transform(X_train_normalized)\n",
    "X_test_pca_20 = pca.transform(X_test_normalized)\n",
    "\n",
    "\n",
    "\n",
    "# Function to make API calls with retry logic for rate limit errors\n",
    "def make_api_call(prompt):\n",
    "    while True:\n",
    "        try:\n",
    "            response = llm.invoke(prompt)\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"Rate limit error: {e}. Retrying in 10 seconds...\")\n",
    "            time.sleep(10)\n",
    "\n",
    "# Set up Groq API credentials and model\n",
    "Groq_Token = \"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\"\n",
    "model_name = \"llama3.1-70b\"\n",
    "llm = ChatGroq(model=groq_models[model_name], api_key=Groq_Token, temperature=0)\n",
    "\n",
    "# Select even samples for few-shot learning examples\n",
    "samples_per_activity = 2\n",
    "few_shot_indices = []\n",
    "activity_count = {i: 0 for i in range(1, 7)}\n",
    "\n",
    "for i, label in enumerate(y_train):\n",
    "    if activity_count[label] < samples_per_activity:\n",
    "        few_shot_indices.append(i)\n",
    "        activity_count[label] += 1\n",
    "    if all(count == samples_per_activity for count in activity_count.values()):\n",
    "        break\n",
    "\n",
    "# Create few-shot examples\n",
    "few_shot_examples = [\n",
    "    {\"input\": X_train[i].tolist(), \"label\": y_train[i]} for i in few_shot_indices\n",
    "]\n",
    "\n",
    "# Function to create a few-shot learning prompt\n",
    "def create_few_shot_prompt(examples, query_input):\n",
    "    description = '''\n",
    "        You are a highly trained human activity classification model.\n",
    "        Each input is a vector containing numerical values that represent transformed features.\n",
    "        The data consists of 20 principal components extracted from the Human Activity Recognition dataset.\n",
    "        Your task is to classify the input vector into one of the following categories:\n",
    "        - 1: WALKING\n",
    "        - 2: WALKING_UPSTAIRS\n",
    "        - 3: WALKING_DOWNSTAIRS\n",
    "        - 4: SITTING \n",
    "        - 5: STANDING\n",
    "        - 6: LAYING\n",
    "\n",
    "        Here are a few examples:\\n\n",
    "    '''\n",
    "    prompt = description\n",
    "    for ex in examples:\n",
    "        example_input = \",\".join(map(str, ex['input']))\n",
    "        prompt += f\"Input: [{example_input}]\\nLabel: {ex['label']}\\n\\n\" \n",
    "        \n",
    "    query_input_str = \",\".join(map(str, query_input))\n",
    "    prompt += f\"Now, classify the following input vector and return ONLY the number.\\nInput: [{query_input_str}]\\nLabel: \"\n",
    "    return prompt\n",
    "\n",
    "# Select 3 samples from each activity for testing\n",
    "samples_per_activity = 3\n",
    "selected_indices = []\n",
    "activity_count = {i: 0 for i in range(1, 7)}\n",
    "\n",
    "for i, label in enumerate(y_test):\n",
    "    if activity_count[label] < samples_per_activity:\n",
    "        selected_indices.append(i)\n",
    "        activity_count[label] += 1\n",
    "    if all(count == samples_per_activity for count in activity_count.values()):\n",
    "        break\n",
    "\n",
    "# Store predictions and true labels\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# Test the selected cases\n",
    "for idx in selected_indices:\n",
    "    query_input = X_test[idx].tolist()\n",
    "    true_label = y_test[idx]\n",
    "    prompt = create_few_shot_prompt(few_shot_examples, query_input)\n",
    "    \n",
    "    # Make the API call and store the prediction\n",
    "    answer = make_api_call(prompt)\n",
    "    predicted_label = int(answer.content.strip())\n",
    "    predictions.append(predicted_label)\n",
    "    true_labels.append(true_label)\n",
    "    \n",
    "    # Print the predicted and true labels\n",
    "    print(f\"Test Case {idx + 1} ({activity_labels[true_label]}):\")\n",
    "    print(f\"Predicted label: {predicted_label}\")\n",
    "    print(f\"True label: {true_label}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Calculate and print accuracy\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Calculate and plot the confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predictions, labels=[1, 2, 3, 4, 5, 6])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['WALKING', 'WALKING_UPSTAIRS', 'WALKING_DOWNSTAIRS', 'SITTING', 'STANDING', 'LAYING'], \n",
    "            yticklabels=['WALKING', 'WALKING_UPSTAIRS', 'WALKING_DOWNSTAIRS', 'SITTING', 'STANDING', 'LAYING'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
