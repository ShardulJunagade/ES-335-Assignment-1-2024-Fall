{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "from langchain_groq import ChatGroq  # Assuming you are using the langchain_groq package\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the model mapping\n",
    "groq_models = {\n",
    "    \"llama3-70b\": \"llama3-70b-8192\",\n",
    "    \"mixtral\": \"mixtral-8x7b-32768\",\n",
    "    \"gemma-7b\": \"gemma-7b-it\",\n",
    "    \"llama3.1-70b\": \"llama-3.1-70b-versatile\",\n",
    "    \"llama3-8b\": \"llama3-8b-8192\",\n",
    "    \"llama3.1-8b\": \"llama-3.1-8b-instant\",\n",
    "    \"gemma-9b\": \"gemma2-9b-it\"\n",
    "}\n",
    "\n",
    "activity_labels = {\n",
    "    1: \"WALKING\",\n",
    "    2: \"WALKING_UPSTAIRS\",\n",
    "    3: \"WALKING_DOWNSTAIRS\",\n",
    "    4: \"SITTING\",\n",
    "    5: \"STANDING\",\n",
    "    6: \"LAYING\"\n",
    "}\n",
    "reverse_activity_labels = {v: k for k, v in activity_labels.items()}\n",
    "\n",
    "print(\"Activity Labels Dictionary: \", reverse_activity_labels)\n",
    "\n",
    "# Load datasets\n",
    "X_train = np.load('../FinalDataset/X_train.npy')\n",
    "X_test = np.load('../FinalDataset/X_test.npy')\n",
    "y_train = np.load('../FinalDataset/y_train.npy')\n",
    "y_test = np.load('../FinalDataset/y_test.npy')\n",
    "\n",
    "#############################################################################################\n",
    "import tsfel\n",
    "\n",
    "# Extract features using TSFEL\n",
    "cfg = tsfel.get_features_by_domain()  # Get all features by default\n",
    "X_train_features = tsfel.time_series_features_extractor(cfg, X_train, verbose=1, fs=50)\n",
    "X_test_features = tsfel.time_series_features_extractor(cfg, X_test, verbose=1, fs=50)\n",
    "print(\"Shape of train data after feature extraction using TSFEL:\", X_train_features.shape)\n",
    "print(\"Shape of test data after feature extraction using TSFEL:\", X_test_features.shape)\n",
    "\n",
    "# Remove highly correlated features\n",
    "correlated_features = tsfel.correlated_features(X_train_features)\n",
    "print(\"Highly correlated features (sample):\", correlated_features[:5])\n",
    "X_train_filtered = X_train_features.drop(correlated_features, axis=1)\n",
    "X_test_filtered = X_test_features.drop(correlated_features, axis=1)\n",
    "print(\"Shape of data after removing correlated features:\", X_train_filtered.shape, X_test_filtered.shape)\n",
    "\n",
    "# Remove low variance features\n",
    "variance_selector = VarianceThreshold(threshold=0)  # Default is 0\n",
    "X_train_reduced = variance_selector.fit_transform(X_train_filtered)\n",
    "X_test_reduced = variance_selector.transform(X_test_filtered)\n",
    "print(\"Shape of data after variance thresholding:\", X_train_reduced.shape, X_test_reduced.shape)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train_reduced)\n",
    "X_test_normalized = scaler.transform(X_test_reduced)\n",
    "print(\"Shape of data after normalization:\", X_train_normalized.shape, X_test_normalized.shape)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=6)\n",
    "X_train_pca = pca.fit_transform(X_train_normalized)\n",
    "X_test_pca = pca.transform(X_test_normalized)\n",
    "\n",
    "print(\"Shape of data after PCA:\", X_train_pca.shape)\n",
    "print(\"Shape of test data after PCA:\", X_test_pca.shape)\n",
    "\n",
    "##################################################################################################\n",
    "\n",
    "# Constants\n",
    "NUM_EXAMPLES_PER_CLASS = 4\n",
    "NUM_SAMPLES = 20\n",
    "MAX_RETRIES = 3\n",
    "RETRY_DELAY = 5\n",
    "\n",
    "# API Keys\n",
    "API_KEYS = [\n",
    "    \"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\"\n",
    "]\n",
    "api_key_index = 0\n",
    "\n",
    "def get_next_api_key():\n",
    "    global api_key_index\n",
    "    key = API_KEYS[api_key_index]\n",
    "    api_key_index = (api_key_index + 1) % len(API_KEYS)\n",
    "    return key\n",
    "\n",
    "def format_data_as_string(data):\n",
    "    return str(data.tolist())\n",
    "\n",
    "def create_few_shot_examples(X, y, activity_labels, num_examples_per_class, num_samples_per_class):\n",
    "    examples = []\n",
    "    for activity, label in activity_labels.items():\n",
    "        indices = np.where(y == activity)[0]\n",
    "        selected_indices = np.random.choice(indices, num_examples_per_class + num_samples_per_class, replace=False)\n",
    "        for idx in selected_indices:\n",
    "            example_data = X[idx]\n",
    "            example_str = format_data_as_string(example_data)\n",
    "            examples.append((example_str, label))\n",
    "    return shuffle(examples)\n",
    "\n",
    "def create_prompt(examples, data_str):\n",
    "    example_strs = \"\\n\".join([f\"    - Example {i+1}: {ex[0]} -> {ex[1]}\" for i, ex in enumerate(examples)])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are a highly trained human activity classification model.\n",
    "\n",
    "    Your task is to analyze the given accelerometer data and classify the human activity into one of the following categories:\n",
    "    - WALKING\n",
    "    - WALKING_UPSTAIRS\n",
    "    - WALKING_DOWNSTAIRS\n",
    "    - SITTING\n",
    "    - STANDING\n",
    "    - LAYING\n",
    "\n",
    "    Here is the accelerometer data provided:\n",
    "    - You have 500 readings, each containing three accelerometer values: (acceleration_x, acceleration_y, acceleration_z).\n",
    "    - The data is collected over a 10-second period at a sampling rate of 50 Hz, which gives 500 readings.\n",
    "    - I have used the TSFEL library to reduce the dataset to 116 features.\n",
    "    Data Format:\n",
    "    - The data is provided as a nested list. Each inner list represents a single reading: (acceleration_x, acceleration_y, acceleration_z).\n",
    "    - The x component represents depth direction, measured in terms of g-force.\n",
    "    - The y component represents sideways direction, measured in terms of g-force.\n",
    "    - The z component represents forward direction, measured in terms of g-force.\n",
    "\n",
    "    Here are a few examples:{example_strs}\n",
    "\n",
    "    Please analyze the examples extensively and provide the most likely activity label for the below data from the list above.\n",
    "\n",
    "    Provide ONLY the classification label (from the given options above) as output.\n",
    "\n",
    "    Data: {data_str}\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "def predict_activity_labels(X_test, examples, activity_labels, reverse_activity_labels, model_name):\n",
    "    predictions = []\n",
    "    for i in range(NUM_SAMPLES):\n",
    "        test_example = X_test[i]\n",
    "        test_data_str = format_data_as_string(test_example)\n",
    "        prompt = create_prompt(examples, test_data_str)\n",
    "        \n",
    "        api_key = get_next_api_key()\n",
    "        llm = ChatGroq(model=groq_models[model_name], api_key=api_key, temperature=0)\n",
    "\n",
    "        for attempt in range(MAX_RETRIES):\n",
    "            try:\n",
    "                response = llm.invoke(prompt)\n",
    "                print(response.usage_metadata)\n",
    "                predicted_label = response.content.strip()\n",
    "                activity_number = reverse_activity_labels.get(predicted_label, -1)\n",
    "                actual_activity = activity_labels[y_test[i]]\n",
    "                print(f\"Predicted Activity: {predicted_label} | Actual Activity: {actual_activity}\")\n",
    "                predictions.append(activity_number)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}. Retrying in {RETRY_DELAY} seconds...\")\n",
    "                time.sleep(RETRY_DELAY)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Create balanced few-shot examples\n",
    "few_shot_examples = create_few_shot_examples(X_train_pca, y_train, activity_labels, NUM_EXAMPLES_PER_CLASS, NUM_SAMPLES)\n",
    "\n",
    "# Predict activities\n",
    "model_name = \"llama3.1-70b\"\n",
    "predicted_labels = predict_activity_labels(X_test_pca, few_shot_examples, activity_labels, reverse_activity_labels, model_name)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_predictions = sum(predicted_labels[i] == y_test[i] for i in range(NUM_SAMPLES))\n",
    "accuracy = correct_predictions / NUM_SAMPLES\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Accuracy Percentage: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "from langchain_groq import ChatGroq  # Assuming you are using the langchain_groq package\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Define the model mapping\n",
    "groq_models = {\n",
    "    \"llama3-70b\": \"llama3-70b-8192\",\n",
    "    \"mixtral\": \"mixtral-8x7b-32768\",\n",
    "    \"gemma-7b\": \"gemma-7b-it\",\n",
    "    \"llama3.1-70b\": \"llama-3.1-70b-versatile\",\n",
    "    \"llama3-8b\": \"llama3-8b-8192\",\n",
    "    \"llama3.1-8b\": \"llama-3.1-8b-instant\",\n",
    "    \"gemma-9b\": \"gemma2-9b-it\"\n",
    "}\n",
    "\n",
    "activity_labels = {\n",
    "    1: \"WALKING\",\n",
    "    2: \"WALKING_UPSTAIRS\",\n",
    "    3: \"WALKING_DOWNSTAIRS\",\n",
    "    4: \"SITTING\",\n",
    "    5: \"STANDING\",\n",
    "    6: \"LAYING\"\n",
    "}\n",
    "reverse_activity_labels = {v: k for k, v in activity_labels.items()}\n",
    "\n",
    "print(\"Activity Labels Dictionary: \", reverse_activity_labels)\n",
    "\n",
    "# Load datasets\n",
    "X_train = np.load('../FinalDataset/X_train.npy')\n",
    "X_test = np.load('../FinalDataset/X_test.npy')\n",
    "y_train = np.load('../FinalDataset/y_train.npy')\n",
    "y_test = np.load('../FinalDataset/y_test.npy')\n",
    "\n",
    "# Feature extraction and processing steps...\n",
    "\n",
    "import tsfel\n",
    "\n",
    "# Extract features using TSFEL\n",
    "cfg = tsfel.get_features_by_domain()  # Get all features by default\n",
    "X_train_features = tsfel.time_series_features_extractor(cfg, X_train, verbose=1, fs=50)\n",
    "X_test_features = tsfel.time_series_features_extractor(cfg, X_test, verbose=1, fs=50)\n",
    "\n",
    "# Remove highly correlated features\n",
    "correlated_features = tsfel.correlated_features(X_train_features)\n",
    "X_train_filtered = X_train_features.drop(correlated_features, axis=1)\n",
    "X_test_filtered = X_test_features.drop(correlated_features, axis=1)\n",
    "\n",
    "# Remove low variance features\n",
    "variance_selector = VarianceThreshold(threshold=0)\n",
    "X_train_reduced = variance_selector.fit_transform(X_train_filtered)\n",
    "X_test_reduced = variance_selector.transform(X_test_filtered)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train_reduced)\n",
    "X_test_normalized = scaler.transform(X_test_reduced)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train_normalized)\n",
    "X_test_pca = pca.transform(X_test_normalized)\n",
    "\n",
    "# Constants\n",
    "NUM_SAMPLES_PER_ACTIVITY = 3\n",
    "NUM_TRAINING_EXAMPLES = 18  # Total number of examples for few-shot learning\n",
    "MAX_RETRIES = 3\n",
    "RETRY_DELAY = 5\n",
    "\n",
    "# API Keys\n",
    "API_KEYS = [\n",
    "    \"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\"\n",
    "]\n",
    "api_key_index = 0\n",
    "\n",
    "def get_next_api_key():\n",
    "    global api_key_index\n",
    "    key = API_KEYS[api_key_index]\n",
    "    api_key_index = (api_key_index + 1) % len(API_KEYS)\n",
    "    return key\n",
    "\n",
    "def format_data_as_string(data):\n",
    "    return str(data.tolist())\n",
    "\n",
    "def create_few_shot_examples(X_train_pca, y_train, activity_labels, num_samples_per_activity):\n",
    "    few_shot_examples = []\n",
    "    for activity, label in activity_labels.items():\n",
    "        indices = np.where(y_train == activity)[0]\n",
    "        selected_indices = np.random.choice(indices, num_samples_per_activity, replace=False)\n",
    "        for idx in selected_indices:\n",
    "            example_data = X_train_pca[idx]\n",
    "            example_str = format_data_as_string(example_data)\n",
    "            few_shot_examples.append((example_str, label))\n",
    "    return shuffle(few_shot_examples)\n",
    "\n",
    "def create_prompt(few_shot_examples, data_str):\n",
    "    example_strs = \"\\n\".join([f\"    - Example {i+1}: {ex[0]} -> {ex[1]}\" for i, ex in enumerate(few_shot_examples)])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are a highly trained human activity classification model.\n",
    "\n",
    "    Your task is to analyze the given accelerometer data and classify the human activity into one of the following categories:\n",
    "    - WALKING\n",
    "    - WALKING_UPSTAIRS\n",
    "    - WALKING_DOWNSTAIRS\n",
    "    - SITTING\n",
    "    - STANDING\n",
    "    - LAYING\n",
    "\n",
    "    Here is the accelerometer data provided:\n",
    "    - You have 500 readings, each containing three accelerometer values: (acceleration_x, acceleration_y, acceleration_z).\n",
    "    - The data is collected over a 10-second period at a sampling rate of 50 Hz, which gives 500 readings.\n",
    "    - I have used the TSFEL library to reduce the dataset to 116 features.\n",
    "    Data Format:\n",
    "    - The data is provided as a nested list. Each inner list represents a single reading: (acceleration_x, acceleration_y, acceleration_z).\n",
    "    - The x component represents depth direction, measured in terms of g-force.\n",
    "    - The y component represents sideways direction, measured in terms of g-force.\n",
    "    - The z component represents forward direction, measured in terms of g-force.\n",
    "\n",
    "    Here are a few examples:{example_strs}\n",
    "\n",
    "    Please analyze the examples extensively and provide the most likely activity label for the below data from the list above.\n",
    "\n",
    "    Provide ONLY the classification label (from the given options above) as output.\n",
    "\n",
    "    Data: {data_str}\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "def create_balanced_subset(X_test_pca, y_test, activity_labels):\n",
    "    min_samples_per_class = min(np.sum(y_test == activity) for activity in activity_labels.keys())\n",
    "    balanced_X_test = []\n",
    "    balanced_y_test = []\n",
    "    \n",
    "    for activity in activity_labels.keys():\n",
    "        indices = np.where(y_test == activity)[0]\n",
    "        selected_indices = np.random.choice(indices, min_samples_per_class, replace=False)\n",
    "        balanced_X_test.extend(X_test_pca[selected_indices])\n",
    "        balanced_y_test.extend(y_test[selected_indices])\n",
    "    \n",
    "    return np.array(balanced_X_test), np.array(balanced_y_test)\n",
    "\n",
    "def predict_activity_labels(X_test_pca, y_test, few_shot_examples, activity_labels, reverse_activity_labels, model_name):\n",
    "    predictions = []\n",
    "    balanced_X_test, balanced_y_test = create_balanced_subset(X_test_pca, y_test, activity_labels)\n",
    "    \n",
    "    # Use exactly 3 samples from each activity for prediction\n",
    "    test_subset_indices = []\n",
    "    for activity in activity_labels.keys():\n",
    "        indices = np.where(balanced_y_test == activity)[0]\n",
    "        selected_indices = np.random.choice(indices, NUM_SAMPLES_PER_ACTIVITY, replace=False)\n",
    "        test_subset_indices.extend(selected_indices)\n",
    "    \n",
    "    test_subset = balanced_X_test[test_subset_indices]\n",
    "    test_subset_labels = balanced_y_test[test_subset_indices]\n",
    "    \n",
    "    for i in range(len(test_subset)):\n",
    "        test_example = test_subset[i]\n",
    "        test_data_str = format_data_as_string(test_example)\n",
    "        prompt = create_prompt(few_shot_examples, test_data_str)\n",
    "        \n",
    "        api_key = get_next_api_key()\n",
    "        llm = ChatGroq(model=groq_models[model_name], api_key=api_key, temperature=0)\n",
    "\n",
    "        for attempt in range(MAX_RETRIES):\n",
    "            try:\n",
    "                response = llm.invoke(prompt)\n",
    "                print(response.usage_metadata)\n",
    "                predicted_label = response.content.strip()\n",
    "                activity_number = reverse_activity_labels.get(predicted_label, -1)\n",
    "                actual_activity = activity_labels[test_subset_labels[i]]\n",
    "                print(f\"Predicted Activity: {predicted_label} | Actual Activity: {actual_activity}\")\n",
    "                predictions.append(activity_number)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}. Retrying in {RETRY_DELAY} seconds...\")\n",
    "                time.sleep(RETRY_DELAY)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Create balanced few-shot examples\n",
    "few_shot_examples = create_few_shot_examples(X_train_pca, y_train, activity_labels, NUM_SAMPLES_PER_ACTIVITY)\n",
    "\n",
    "# Predict activities\n",
    "model_name = \"llama3.1-70b\"\n",
    "predicted_labels = predict_activity_labels(X_test_pca, y_test, few_shot_examples, activity_labels, reverse_activity_labels, model_name)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_predictions = sum(predicted_labels[i] == y_test[i] for i in range(len(predicted_labels)))\n",
    "accuracy = correct_predictions / len(predicted_labels)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Accuracy Percentage: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activity Labels Dictionary:  {'WALKING': 1, 'WALKING_UPSTAIRS': 2, 'WALKING_DOWNSTAIRS': 3, 'SITTING': 4, 'STANDING': 5, 'LAYING': 6}\n",
      "*** Feature extraction started ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "              <p>\n",
       "                  Progress: 0% Complete\n",
       "              <p/>\n",
       "              <progress\n",
       "                  value='0'\n",
       "                  max='126',\n",
       "                  style='width: 25%',\n",
       "              >\n",
       "                  0\n",
       "              </progress>\n",
       "\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Feature extraction finished ***\n",
      "*** Feature extraction started ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "              <p>\n",
       "                  Progress: 100% Complete\n",
       "              <p/>\n",
       "              <progress\n",
       "                  value='54'\n",
       "                  max='54',\n",
       "                  style='width: 25%',\n",
       "              >\n",
       "                  54\n",
       "              </progress>\n",
       "\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Feature extraction finished ***\n",
      "{'input_tokens': 895, 'output_tokens': 4, 'total_tokens': 899}\n",
      "Predicted Activity: WALKING | Actual Activity: WALKING\n",
      "{'input_tokens': 896, 'output_tokens': 4, 'total_tokens': 900}\n",
      "Predicted Activity: WALKING | Actual Activity: WALKING\n",
      "{'input_tokens': 896, 'output_tokens': 8, 'total_tokens': 904}\n",
      "Predicted Activity: WALKING_UPSTAIRS | Actual Activity: WALKING\n",
      "{'input_tokens': 895, 'output_tokens': 4, 'total_tokens': 899}\n",
      "Predicted Activity: WALKING | Actual Activity: WALKING_UPSTAIRS\n",
      "{'input_tokens': 896, 'output_tokens': 4, 'total_tokens': 900}\n",
      "Predicted Activity: WALKING | Actual Activity: WALKING_UPSTAIRS\n",
      "{'input_tokens': 896, 'output_tokens': 4, 'total_tokens': 900}\n",
      "Predicted Activity: WALKING | Actual Activity: WALKING_UPSTAIRS\n",
      "{'input_tokens': 896, 'output_tokens': 8, 'total_tokens': 904}\n",
      "Predicted Activity: WALKING_DOWNSTAIRS | Actual Activity: WALKING_DOWNSTAIRS\n",
      "{'input_tokens': 896, 'output_tokens': 8, 'total_tokens': 904}\n",
      "Predicted Activity: WALKING_UPSTAIRS | Actual Activity: WALKING_DOWNSTAIRS\n",
      "{'input_tokens': 896, 'output_tokens': 8, 'total_tokens': 904}\n",
      "Predicted Activity: WALKING_UPSTAIRS | Actual Activity: WALKING_DOWNSTAIRS\n",
      "{'input_tokens': 896, 'output_tokens': 3, 'total_tokens': 899}\n",
      "Predicted Activity: LAYING | Actual Activity: SITTING\n",
      "{'input_tokens': 896, 'output_tokens': 3, 'total_tokens': 899}\n",
      "Predicted Activity: LAYING | Actual Activity: SITTING\n",
      "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-70b-versatile` in organization `org_01j62b6cjzekat7nb1bm92fyws` on : Limit 1000000, Used 1000167, Requested 672. Please try again in 1m12.5014s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}. Retrying in 5 seconds...\n",
      "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-70b-versatile` in organization `org_01j62b6cjzekat7nb1bm92fyws` on : Limit 1000000, Used 1000105, Requested 672. Please try again in 1m7.1564s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}. Retrying in 5 seconds...\n",
      "Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-70b-versatile` in organization `org_01j62b6cjzekat7nb1bm92fyws` on : Limit 1000000, Used 1000044, Requested 672. Please try again in 1m1.8704s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}. Retrying in 5 seconds...\n",
      "{'input_tokens': 896, 'output_tokens': 3, 'total_tokens': 899}\n",
      "Predicted Activity: LAYING | Actual Activity: STANDING\n",
      "{'input_tokens': 895, 'output_tokens': 3, 'total_tokens': 898}\n",
      "Predicted Activity: LAYING | Actual Activity: STANDING\n",
      "{'input_tokens': 896, 'output_tokens': 3, 'total_tokens': 899}\n",
      "Predicted Activity: STANDING | Actual Activity: STANDING\n",
      "{'input_tokens': 895, 'output_tokens': 3, 'total_tokens': 898}\n",
      "Predicted Activity: LAYING | Actual Activity: LAYING\n",
      "{'input_tokens': 896, 'output_tokens': 4, 'total_tokens': 900}\n",
      "Predicted Activity: WALKING | Actual Activity: LAYING\n",
      "{'input_tokens': 895, 'output_tokens': 3, 'total_tokens': 898}\n",
      "Predicted Activity: LAYING | Actual Activity: LAYING\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from langchain_groq import ChatGroq  # Assuming you are using the langchain_groq package\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Define the model mapping\n",
    "groq_models = {\n",
    "    \"llama3-70b\": \"llama3-70b-8192\",\n",
    "    \"mixtral\": \"mixtral-8x7b-32768\",\n",
    "    \"gemma-7b\": \"gemma-7b-it\",\n",
    "    \"llama3.1-70b\": \"llama-3.1-70b-versatile\",\n",
    "    \"llama3-8b\": \"llama3-8b-8192\",\n",
    "    \"llama3.1-8b\": \"llama-3.1-8b-instant\",\n",
    "    \"gemma-9b\": \"gemma2-9b-it\"\n",
    "}\n",
    "\n",
    "activity_labels = {\n",
    "    1: \"WALKING\",\n",
    "    2: \"WALKING_UPSTAIRS\",\n",
    "    3: \"WALKING_DOWNSTAIRS\",\n",
    "    4: \"SITTING\",\n",
    "    5: \"STANDING\",\n",
    "    6: \"LAYING\"\n",
    "}\n",
    "reverse_activity_labels = {v: k for k, v in activity_labels.items()}\n",
    "\n",
    "print(\"Activity Labels Dictionary: \", reverse_activity_labels)\n",
    "\n",
    "# Load datasets\n",
    "X_train = np.load('../FinalDataset/X_train.npy')\n",
    "X_test = np.load('../FinalDataset/X_test.npy')\n",
    "y_train = np.load('../FinalDataset/y_train.npy')\n",
    "y_test = np.load('../FinalDataset/y_test.npy')\n",
    "\n",
    "# Feature extraction and processing steps...\n",
    "\n",
    "import tsfel\n",
    "\n",
    "# Extract features using TSFEL\n",
    "cfg = tsfel.get_features_by_domain()  # Get all features by default\n",
    "X_train_features = tsfel.time_series_features_extractor(cfg, X_train, verbose=1, fs=50)\n",
    "X_test_features = tsfel.time_series_features_extractor(cfg, X_test, verbose=1, fs=50)\n",
    "\n",
    "# Remove highly correlated features\n",
    "correlated_features = tsfel.correlated_features(X_train_features)\n",
    "X_train_filtered = X_train_features.drop(correlated_features, axis=1)\n",
    "X_test_filtered = X_test_features.drop(correlated_features, axis=1)\n",
    "\n",
    "# Remove low variance features\n",
    "variance_selector = VarianceThreshold(threshold=0)\n",
    "X_train_reduced = variance_selector.fit_transform(X_train_filtered)\n",
    "X_test_reduced = variance_selector.transform(X_test_filtered)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train_reduced)\n",
    "X_test_normalized = scaler.transform(X_test_reduced)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train_normalized)\n",
    "X_test_pca = pca.transform(X_test_normalized)\n",
    "\n",
    "# Constants\n",
    "NUM_SAMPLES_PER_ACTIVITY = 3\n",
    "NUM_TRAINING_EXAMPLES = 18 # Total number of examples for few-shot learning\n",
    "MAX_RETRIES = 3\n",
    "RETRY_DELAY = 5\n",
    "\n",
    "# # API Keys\n",
    "# API_KEYS = [\n",
    "#     \"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\"\n",
    "# ]\n",
    "\n",
    "API_KEYS = [\"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\",\n",
    "               \"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\",\n",
    "               \"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\",\n",
    "                \"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\"\n",
    "]\n",
    "api_key_index = 0\n",
    "\n",
    "def get_next_api_key():\n",
    "    global api_key_index\n",
    "    key = API_KEYS[api_key_index]\n",
    "    api_key_index = (api_key_index + 1) % len(API_KEYS)\n",
    "    return key\n",
    "\n",
    "def format_data_as_string(data):\n",
    "    return str(data.tolist())\n",
    "\n",
    "def create_few_shot_examples(X_train_pca, y_train, activity_labels, num_samples_per_activity):\n",
    "    few_shot_examples = []\n",
    "    for activity, label in activity_labels.items():\n",
    "        indices = np.where(y_train == activity)[0]\n",
    "        selected_indices = np.random.choice(indices, num_samples_per_activity, replace=False)\n",
    "        for idx in selected_indices:\n",
    "            example_data = X_train_pca[idx]\n",
    "            example_str = format_data_as_string(example_data)\n",
    "            few_shot_examples.append((example_str, label))\n",
    "    return shuffle(few_shot_examples)\n",
    "\n",
    "def create_prompt(few_shot_examples, data_str):\n",
    "    example_strs = \"\\n\".join([f\"    - Example {i+1}: {ex[0]} -> {ex[1]}\" for i, ex in enumerate(few_shot_examples)])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are a highly trained human activity classification model.\n",
    "\n",
    "    Your task is to analyze the given accelerometer data and classify the human activity into one of the following categories:\n",
    "    - WALKING\n",
    "    - WALKING_UPSTAIRS\n",
    "    - WALKING_DOWNSTAIRS\n",
    "    - SITTING\n",
    "    - STANDING\n",
    "    - LAYING\n",
    "\n",
    "    Here is the accelerometer data provided:\n",
    "    - You have 500 readings, each containing three accelerometer values: (acceleration_x, acceleration_y, acceleration_z).\n",
    "    - The data is collected over a 10-second period at a sampling rate of 50 Hz, which gives 500 readings.\n",
    "    - I have used the TSFEL library to reduce the dataset to 116 features.\n",
    "    Data Format:\n",
    "    - The data is provided as a nested list. Each inner list represents a single reading: (acceleration_x, acceleration_y, acceleration_z).\n",
    "    - The x component represents depth direction, measured in terms of g-force.\n",
    "    - The y component represents sideways direction, measured in terms of g-force.\n",
    "    - The z component represents forward direction, measured in terms of g-force.\n",
    "\n",
    "    Here are a few examples:{example_strs}\n",
    "\n",
    "    Please analyze the examples extensively and provide the most likely activity label for the below data from the list above.\n",
    "\n",
    "    Provide ONLY the classification label (from the given options above) as output.\n",
    "\n",
    "    Data: {data_str}\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "def create_balanced_subset(X_test_pca, y_test, activity_labels):\n",
    "    min_samples_per_class = min(np.sum(y_test == activity) for activity in activity_labels.keys())\n",
    "    balanced_X_test = []\n",
    "    balanced_y_test = []\n",
    "    \n",
    "    for activity in activity_labels.keys():\n",
    "        indices = np.where(y_test == activity)[0]\n",
    "        selected_indices = np.random.choice(indices, min_samples_per_class, replace=False)\n",
    "        balanced_X_test.extend(X_test_pca[selected_indices])\n",
    "        balanced_y_test.extend(y_test[selected_indices])\n",
    "    \n",
    "    return np.array(balanced_X_test), np.array(balanced_y_test)\n",
    "\n",
    "def predict_activity_labels(X_test_pca, y_test, few_shot_examples, activity_labels, reverse_activity_labels, model_name):\n",
    "    predictions = []\n",
    "    balanced_X_test, balanced_y_test = create_balanced_subset(X_test_pca, y_test, activity_labels)\n",
    "    \n",
    "    # Use exactly 3 samples from each activity for prediction\n",
    "    test_subset_indices = []\n",
    "    for activity in activity_labels.keys():\n",
    "        indices = np.where(balanced_y_test == activity)[0]\n",
    "        selected_indices = np.random.choice(indices, NUM_SAMPLES_PER_ACTIVITY, replace=False)\n",
    "        test_subset_indices.extend(selected_indices)\n",
    "    \n",
    "    test_subset = balanced_X_test[test_subset_indices]\n",
    "    test_subset_labels = balanced_y_test[test_subset_indices]\n",
    "    \n",
    "    for i in range(len(test_subset)):\n",
    "        test_example = test_subset[i]\n",
    "        test_data_str = format_data_as_string(test_example)\n",
    "        prompt = create_prompt(few_shot_examples, test_data_str)\n",
    "        \n",
    "        api_key = get_next_api_key()\n",
    "        llm = ChatGroq(model=groq_models[model_name], api_key=api_key, temperature=0)\n",
    "\n",
    "        for attempt in range(MAX_RETRIES):\n",
    "            try:\n",
    "                response = llm.invoke(prompt)\n",
    "                print(response.usage_metadata)\n",
    "                predicted_label = response.content.strip()\n",
    "                activity_number = reverse_activity_labels.get(predicted_label, -1)\n",
    "                actual_activity = activity_labels[test_subset_labels[i]]\n",
    "                print(f\"Predicted Activity: {predicted_label} | Actual Activity: {actual_activity}\")\n",
    "                predictions.append(activity_number)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}. Retrying in {RETRY_DELAY} seconds...\")\n",
    "                time.sleep(RETRY_DELAY)\n",
    "    \n",
    "    return np.array(predictions), balanced_y_test[test_subset_indices]\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, activity_labels):\n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(activity_labels.keys()))\n",
    "    \n",
    "    # Create a heatmap for the confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(activity_labels.values()))\n",
    "    disp.plot(cmap=plt.cm.Blues, values_format='d')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "# Create balanced few-shot examples\n",
    "few_shot_examples = create_few_shot_examples(X_train_pca, y_train, activity_labels, NUM_SAMPLES_PER_ACTIVITY)\n",
    "\n",
    "# Predict activities\n",
    "model_name = \"llama3.1-70b\"\n",
    "predicted_labels, true_labels = predict_activity_labels(X_test_pca, y_test, few_shot_examples, activity_labels, reverse_activity_labels, model_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.47\n",
      "Accuracy Percentage: 47.06%\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [18, 17]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy Percentage: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m plot_confusion_matrix(true_labels, predicted_labels, activity_labels)\n",
      "Cell \u001b[1;32mIn[1], line 195\u001b[0m, in \u001b[0;36mplot_confusion_matrix\u001b[1;34m(y_true, y_pred, activity_labels)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_confusion_matrix\u001b[39m(y_true, y_pred, activity_labels):\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;66;03m# Create confusion matrix\u001b[39;00m\n\u001b[1;32m--> 195\u001b[0m     cm \u001b[38;5;241m=\u001b[39m confusion_matrix(y_true, y_pred, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(activity_labels\u001b[38;5;241m.\u001b[39mkeys()))\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;66;03m# Create a heatmap for the confusion matrix\u001b[39;00m\n\u001b[0;32m    198\u001b[0m     plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Soham\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Soham\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:326\u001b[0m, in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m    232\u001b[0m     {\n\u001b[0;32m    233\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    242\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    243\u001b[0m ):\n\u001b[0;32m    244\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute confusion matrix to evaluate the accuracy of a classification.\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \n\u001b[0;32m    246\u001b[0m \u001b[38;5;124;03m    By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;124;03m    (0, 2, 1, 1)\u001b[39;00m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 326\u001b[0m     y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m y_type)\n",
      "File \u001b[1;32mc:\\Users\\Soham\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:84\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[0;32m     58\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m     check_consistent_length(y_true, y_pred)\n\u001b[0;32m     85\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     86\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Soham\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:409\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    407\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 409\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    410\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    411\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    412\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [18, 17]"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "correct_predictions = sum(predicted_labels[i] == true_labels[i] for i in range(len(predicted_labels)))\n",
    "accuracy = correct_predictions / len(predicted_labels)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Accuracy Percentage: {accuracy * 100:.2f}%\")\n",
    "# plot_confusion_matrix(true_labels, predicted_labels, activity_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "from langchain_groq import ChatGroq  # Assuming you are using the langchain_groq package\n",
    "\n",
    "\n",
    "# Define the model mapping\n",
    "groq_models = {\n",
    "    \"llama3-70b\": \"llama3-70b-8192\",\n",
    "    \"mixtral\": \"mixtral-8x7b-32768\",\n",
    "    \"gemma-7b\": \"gemma-7b-it\",\n",
    "    \"llama3.1-70b\": \"llama-3.1-70b-versatile\",\n",
    "    \"llama3-8b\": \"llama3-8b-8192\",\n",
    "    \"llama3.1-8b\": \"llama-3.1-8b-instant\",\n",
    "    \"gemma-9b\": \"gemma2-9b-it\"\n",
    "}\n",
    "\n",
    "activity_labels = {\n",
    "    1: \"WALKING\",\n",
    "    2: \"WALKING_UPSTAIRS\",\n",
    "    3: \"WALKING_DOWNSTAIRS\",\n",
    "    4: \"SITTING\",\n",
    "    5: \"STANDING\",\n",
    "    6: \"LAYING\"\n",
    "}\n",
    "reverse_activity_labels = {v: k for k, v in activity_labels.items()}\n",
    "\n",
    "print(\"Activity Labels Dictionary: \", reverse_activity_labels)\n",
    "\n",
    "# Load datasets\n",
    "X_train = np.load('../FinalDataset/X_train.npy')\n",
    "X_test = np.load('../FinalDataset/X_test.npy')\n",
    "y_train = np.load('../FinalDataset/y_train.npy')\n",
    "y_test = np.load('../FinalDataset/y_test.npy')\n",
    "# Load data\n",
    "# X_train_tsfel_reduced = np.load('../FinalDataset/X_train_tsfel_reduced.npy')\n",
    "# X_test_tsfel_reduced = np.load('../FinalDataset/X_test_tsfel_reduced.npy')\n",
    "\n",
    "print(\"Training data shape: \", X_train_tsfel_pca.shape)\n",
    "print(\"Testing data shape: \", X_test_tsfel_pca.shape)\n",
    "\n",
    "# Constants\n",
    "num_examples_per_class = 5\n",
    "num_samples = 20\n",
    "max_retries = 3\n",
    "retry_delay = 5\n",
    "\n",
    "def shuffle_data(X_test, y_test):\n",
    "    # Generate a random permutation of the indices\n",
    "    permutation = np.random.permutation(len(X_test))\n",
    "    \n",
    "    # Apply the permutation to shuffle both X_test and y_test\n",
    "    X_test_shuffled = X_test[permutation]\n",
    "    y_test_shuffled = y_test[permutation]\n",
    "    \n",
    "    return X_test_shuffled, y_test_shuffled\n",
    "\n",
    "# Example usage:\n",
    "X_test_tsfel_pca_shuffled, y_test_shuffled = shuffle_data(X_test_tsfel_pca, y_test)\n",
    "\n",
    "# API Keys and Index\n",
    "Groq_Tokens = [\"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\"]\n",
    "current_key_index = 0\n",
    "\n",
    "def get_next_api_key():\n",
    "    global current_key_index\n",
    "    api_key = Groq_Tokens[current_key_index]\n",
    "    current_key_index = (current_key_index + 1) % len(Groq_Tokens)\n",
    "    return api_key\n",
    "\n",
    "def format_data_for_prompt(data):\n",
    "    # Example formatting function; adjust as needed\n",
    "    return str(data.tolist())\n",
    "\n",
    "# def create_few_shot_examples(X_train, y_train, activity_dict, num_examples_per_class):\n",
    "#     examples = []\n",
    "#     for activity, label in activity_dict.items():\n",
    "#         class_indices = np.where(y_train == activity)[0]\n",
    "#         selected_indices = np.random.choice(class_indices, num_examples_per_class, replace=False)\n",
    "#         for idx in selected_indices:\n",
    "#             data_example = X_train[idx]\n",
    "#             data_str = format_data_for_prompt(data_example)\n",
    "#             examples.append((data_str, label))\n",
    "#     return shuffle(examples)\n",
    "\n",
    "def add_class_examples(X_train, y_train, activity_dict, num_samples_per_class=4):\n",
    "    examples = []\n",
    "    for activity, label in activity_dict.items():\n",
    "        class_indices = np.where(y_train == activity)[0]\n",
    "        class_samples = np.random.choice(class_indices, num_samples_per_class, replace=False)\n",
    "        for idx in class_samples:\n",
    "            data_example = X_train[idx]\n",
    "            data_str = format_data_for_prompt(data_example)\n",
    "            examples.append((data_str, label))\n",
    "    return shuffle(examples)\n",
    "\n",
    "def generate_prompt(examples, data_str):\n",
    "    output = \"\\n\"\n",
    "    for j, (example_input, example_output) in enumerate(examples):\n",
    "        output += f\"    - Example {j+1}: {example_input} -> {example_output}\\n\"\n",
    "    # print(output)\n",
    "    prompt = f\"\"\"\n",
    "    You are a highly trained human activity classification model.\n",
    "\n",
    "    Your task is to analyze the given accelerometer data and classify the human activity into one of the following categories:\n",
    "    - WALKING\n",
    "    - WALKING_UPSTAIRS\n",
    "    - WALKING_DOWNSTAIRS\n",
    "    - SITTING\n",
    "    - STANDING\n",
    "    - LAYING\n",
    "\n",
    "    Here is the processed data provided:\n",
    "    - You have two principal components: (principal_component_1, principal_component_2).\n",
    "    - The data was originally derived from accelerometer readings collected over a 10-second period at a sampling rate of 50 Hz.\n",
    "    - The principal components capture the most significant variance in the accelerometer data after dimensionality reduction using PCA.\n",
    "\n",
    "    Here are a few examples:{output}\n",
    "    \n",
    "    Please analyze the examples extensively and provide the most likely activity label for the below data from the list above.\n",
    "    \n",
    "    Provide ONLY the classification label (from the given options above) as output.\n",
    "\n",
    "    Data: {data_str}\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "def predict_activity(X_test, examples, activity_dict, activity_reverse_dict, model_name):\n",
    "    predictions = []\n",
    "    count = 0\n",
    "    for i in range(num_samples):\n",
    "        X_i = X_test[i]\n",
    "        data_str = format_data_for_prompt(X_i)\n",
    "        prompt = generate_prompt(examples, data_str)\n",
    "        \n",
    "        api_key = get_next_api_key()\n",
    "        llm = ChatGroq(model=groq_models[model_name], api_key=api_key, temperature=0)\n",
    "\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                answer = llm.invoke(prompt)\n",
    "                count+=answer.usage_metadata['total_tokens']\n",
    "                predicted_activity = answer.content.strip()\n",
    "                activity_number = activity_reverse_dict.get(predicted_activity, -1)\n",
    "                # Get actual label\n",
    "                actual_activity = activity_dict[y_test_shuffled[i]]\n",
    "                # Print predicted and actual activity\n",
    "                print(f\"Predicted Activity: {predicted_activity} | Actual Activity: {actual_activity}\")\n",
    "                predictions.append(activity_number)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}. Retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "    print(\"Total Tokens Used: \",count)\n",
    "    return predictions\n",
    "\n",
    "# Create balanced few-shot examples\n",
    "# examples = create_few_shot_examples(X_train_tsfel_pca, y_train, activity_dict, num_examples_per_class)\n",
    "examples = add_class_examples(X_train_tsfel_pca, y_train, activity_dict, num_samples_per_class=10)\n",
    "\n",
    "model_name = \"llama3.1-70b\"\n",
    "# Predict activities\n",
    "predictions = predict_activity(X_test_tsfel_pca_shuffled, examples, activity_dict, activity_reverse_dict, model_name)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct = sum(pred == y_test[i] for i, pred in enumerate(predictions))\n",
    "accuracy = correct / len(predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Accuracy Percentage: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
